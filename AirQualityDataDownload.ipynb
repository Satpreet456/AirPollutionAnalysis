{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as urllib\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.chdir('E:/Research/Processing Datasets')\n",
    "\n",
    "#Extract countries available in pollutant release dataset\n",
    "facilities = pd.read_csv('E:\\Research\\Datasets\\Pollutant emissions data\\E-PRTR_database_v16_csv\\dbo.PUBLISH_FACILITYREPORT.csv', encoding = \"ISO-8859-1\")\n",
    "countryCodes = set(facilities['CountryCode'])\n",
    "\n",
    "#Find common pollutants reported by all countries\n",
    "#Checked download urls present in text file downloaded through e-reporting API\n",
    "data = []\n",
    "myfile = open(\"dataset _download_links.txt\", \"r\")\n",
    "lines = myfile.read().split('\\n')\n",
    "#For every record, add new item in list\n",
    "for index,line in enumerate(lines):\n",
    "    name = line.split('/')[-1]\n",
    "    if name[0:2] in countryCodes:\n",
    "        data.append({\"Number\": index, \"Country\": name[0:2], \"Pollutant\":name.split('_')[1]})\n",
    "        \n",
    "myfile.close()\n",
    "#Drop dups and group counts\n",
    "df1 = pd.DataFrame(data).drop_duplicates(['Country','Pollutant'], keep= 'last')\n",
    "df2 = pd.DataFrame(columns = ['No_Reporting_Countries'])\n",
    "df2['No_Reporting_Countries'] = df1.groupby('Pollutant').apply(lambda x: len(x['Country'].unique()))\n",
    "\n",
    "#Filter pollutants reported by all countries\n",
    "reported_pols = df2[df2['No_Reporting_Countries'] == 30]\n",
    "polNames = pd.DataFrame([{'Pollutant':'1', 'Pollutant_Name':'SO2'},{'Pollutant':'10', 'Pollutant_Name':'CO'},{'Pollutant':'20', 'Pollutant_Name':'C6H6'},{'Pollutant':'5', 'Pollutant_Name':'PM10'},{'Pollutant':'6001', 'Pollutant_Name':'PM2.5'},{'Pollutant':'7', 'Pollutant_Name':'O3'},{'Pollutant':'8', 'Pollutant_Name':'NO2'},{'Pollutant':'9', 'Pollutant_Name':'Nox as No2'}])\n",
    "reported_pols = pd.merge(reported_pols, polNames, on='Pollutant')\n",
    "reported_pols[['Pollutant_Name', 'No_Reporting_Countries']]\n",
    "\n",
    "#From all countries and all polutants datasets urls, filter out and save urls for above pollutants and countries\n",
    "myfile = open(\"dataset _download_links.txt\", \"r\")\n",
    "lines = myfile.read().split('\\n')\n",
    "filtered_urls = []\n",
    "for url in lines:\n",
    "    name = url.split('/')[-1]\n",
    "    if name[0:2] in final_all_countries and name.split('_')[1] in final_pollutants:\n",
    "        filtered_urls.append(url)\n",
    "        \n",
    "with open('filtered_dataset_urls.txt', 'w') as textFile:\n",
    "    for url in filtered_urls:\n",
    "        textFile.write(\"%s\\n\" % url)\n",
    "        \n",
    "#Open filtered dataset urls file and download all datasets in a directory\n",
    "filteredDatasetFile = open(\"filtered_dataset_urls.txt\", \"r\")\n",
    "lines = filteredDatasetFile.read().split('\\n')\n",
    "downfile = urllib.URLopener()\n",
    "os.chdir('E:/Research/Processing Datasets/Air quality datasets')\n",
    "for ind, line in enumerate(lines):\n",
    "    name = line.split('/')[-1]\n",
    "    try:\n",
    "        downfile.retrieve(line, name)\n",
    "        print('Downloaded File ', ind, ':', name)\n",
    "    except Exception:\n",
    "        #For any exception like network error, report file, skip processing, pause for 30 seconds and then retry downloading next file\n",
    "        print('******An exception occured while downloading file [', ind, ':', line, ']...Retrying in 30 seconds: ******')\n",
    "        time.sleep(30)\n",
    "        continue\n",
    "\n",
    "filteredDatasetFile.close()\n",
    "\n",
    "#Try downloading files failed with exceptionsza\n",
    "print('Starting to process to download all URL files')\n",
    "import time\n",
    "import os\n",
    "import urllib.request as urllib\n",
    "os.chdir('E:/Research/Processing Datasets')\n",
    "start = time.process_time\n",
    "outputFile = open(\"Download_dataset_output.txt\", \"r\")\n",
    "lines = outputFile.read().split('\\n')\n",
    "\n",
    "failedUrls = [line for line in lines if line.startswith('***')]\n",
    "print(len(failedUrls))\n",
    "downfile = urllib.URLopener()\n",
    "os.chdir('E:/Research/Processing Datasets/Air quality datasets')\n",
    "for ind, line in enumerate(failedUrls):\n",
    "    try:\n",
    "        url = line.split(\" \")[9]\n",
    "        name = url.split('/')[-1]\n",
    "        downfile.retrieve(url, name)\n",
    "        print('Downloaded File ', ind, ':', name)\n",
    "    except Exception:\n",
    "        print('******An exception occured while downloading file [', ind, ':', url, ']...Retrying in 30 seconds: ******')\n",
    "        time.sleep(30)\n",
    "        continue\n",
    "\n",
    "outputFile.close()\n",
    "print('Processed downloading all files in', time.process_time() - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
