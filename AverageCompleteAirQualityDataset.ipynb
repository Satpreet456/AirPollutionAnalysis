{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Script to parse downloaded files for period 2013-2018 clean it, average concentration and store data in a dictionary to save in a csv later\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "os.chdir('E:/Research/Processing Datasets/Air quality datasets')\n",
    "airQualityData = {}\n",
    "\n",
    "for ind, filename in enumerate(os.listdir(os.getcwd())):\n",
    "    try:\n",
    "        #Try reading files with UTF 16 format\n",
    "        try:\n",
    "            df = pd.read_csv(filename, error_bad_lines=False, encoding = \"utf-16\")\n",
    "        except UnicodeError:\n",
    "            #Try reading file without mention encoding\n",
    "            df = pd.read_csv(filename, error_bad_lines=False)\n",
    "        splits = filename.split(\"_\")\n",
    "        #Not process 2019 files\n",
    "        if \"2019\" != splits[3]:\n",
    "            #Report if namespaces and sampling point more than stations\n",
    "            if len(df['Namespace'].unique()) > len(df['AirQualityStationEoICode'].unique()):\n",
    "                print(\"Namespace more than stations file : \", filename)\n",
    "            if len(df['SamplingPoint'].unique()) > len(df['AirQualityStationEoICode'].unique()):\n",
    "                print(\"SamplingPoint more than stations file : \", filename) \n",
    "\n",
    "            #Filter out invalid column unique values\n",
    "            stationCodes = [code for code in df['AirQualityStationEoICode'].unique() if type(code)==str]\n",
    "            pollutants = [pol for pol in df['AirPollutant'].unique() if type(pol)==str and not pol.replace('.', '').isdigit()]\n",
    "            avgTimes = [time for time in df['AveragingTime'].unique() if type(time)==str]\n",
    "            UOMs = [unit for unit in df['UnitOfMeasurement'].unique() if type(unit)==str]\n",
    "            \n",
    "            #Store average for each type\n",
    "            for stationCode in stationCodes:\n",
    "                for pollutant in pollutants:        \n",
    "                    for avgTime in avgTimes:\n",
    "                        for unit in UOMs:\n",
    "                            key = splits[0] + splits[3] + stationCode + df['Namespace'].iloc[0] + df['SamplingPoint'].iloc[0] + pollutant + avgTime + unit\n",
    "                            temp = df[(df['AirPollutant'] == pollutant) & (df['AirQualityStationEoICode'] == stationCode) & (df['AveragingTime'] == avgTime) & (df['UnitOfMeasurement'] == unit)]\n",
    "                            avgConcentration =  temp['Concentration'].astype(float).mean()\n",
    "                            if key in airQualityData:\n",
    "                                print(\"****************************************************\")\n",
    "                                print(\"found existing key : \", key, \" file :\", filename)\n",
    "                                airQualityData.get(key)[pollutant] = avgConcentration\n",
    "                            else:\n",
    "                                airQualityData[key] = {'Countrycode' : df['Countrycode'].iloc[0], 'Year': splits[3], 'AirQualityStationEoICode': stationCode, 'Namespace' : df['Namespace'].iloc[0] , 'SamplingPoint' : df['SamplingPoint'].iloc[0], 'AveragingTime' : avgTime, 'UnitOfMeasurement' : unit, pollutant: avgConcentration, 'ReportedFile':filename}\n",
    "    except Exception:\n",
    "        print('Exception while processing file ', ind, ' - ', filename)\n",
    "        print(traceback.format_exc())\n",
    "        continue\n",
    "        \n",
    "#Convert to data frame\n",
    "pd.DataFrame.from_dict(airQualityData, orient='index').to_csv(\"E:/Research/Processing Datasets/AirQualityMergedDataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge with locations dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "os.chdir('E:/Research/Datasets/Pollutant emissions data/')\n",
    "#Read pollutant release facilities\n",
    "facilities = pd.read_csv('E:/Research/Datasets/Pollutant emissions data/dbo.PUBLISH_FACILITYREPORT.csv', encoding = \"ISO-8859-1\")\n",
    "#Filter out not needed columns\n",
    "facilities = facilities[['FacilityReportID', 'PollutantReleaseAndTransferReportID','FacilityID', 'NationalID', 'FacilityName', 'CountryCode', 'CountryName', 'Lat', 'Long']]\n",
    "\n",
    "os.chdir('E:/Research/Processing Datasets')\n",
    "airQualityMergedDataset = pd.read_excel(open('AirQualityMergedDataset.xlsx','rb'), sheet_name='AirQualityMergedDataset')\n",
    "airQualityMergedDataset.rename(columns={'Countrycode': 'CountryCode'}, inplace=True)\n",
    "#Filter out countries not present in pollutant release dataset\n",
    "airQualityMergedDataset = pd.merge(facilities[['CountryCode', 'CountryName']].drop_duplicates(), airQualityMergedDataset, on='CountryCode')\n",
    "airQualityMergedDataset = airQualityMergedDataset[[\"Year\", \"CountryCode\", \"CountryName\", \"AirQualityStationEoICode\", \"Namespace\", \"SamplingPoint\", \"AveragingTime\", \"UnitOfMeasurement\", \"SO2\", \"C6H6\", \"CO\", \"PM10\", \"PM2.5\", \"O3\", \"NO2\"]]\n",
    "#Remove errored unit of measurement rows\n",
    "airQualityMergedDataset = airQualityMergedDataset.loc[airQualityMergedDataset['UnitOfMeasurement'].isin([uni for uni in airQualityMergedDataset['UnitOfMeasurement'].unique() if type(uni) == str and re.search('[a-zA-Z]', uni)])]\n",
    "#Filter only hourly data values\n",
    "airQualityMergedDataset = airQualityMergedDataset[airQualityMergedDataset['AveragingTime'] == 'hour']\n",
    "#Remove rows with all blank air pollutant values\n",
    "airQualityMergedDataset = airQualityMergedDataset.dropna(subset=[\"SO2\", \"C6H6\", \"CO\", \"PM10\", \"PM2.5\", \"O3\", \"NO2\"], how='all')\n",
    "#group by station EOI code and calculate mean\n",
    "airQualityStationsMeanReadings = airQualityMergedDataset.groupby([\"AirQualityStationEoICode\", \"Year\", \"CountryCode\", \"CountryName\"], as_index=False).agg({'SO2': 'mean', 'C6H6': 'mean', 'CO': 'mean', 'PM10': 'mean', 'O3': 'mean', 'NO2': 'mean',  'PM2.5': 'mean'})\n",
    "\n",
    "#read locations dataset\n",
    "stationsLocationsDataset = pd.read_csv('AirQualityStations.csv')\n",
    "stationsLocationsDataset.rename(columns={'EoICode': 'AirQualityStationEoICode'}, inplace=True)\n",
    "stationsLocationsDataset = stationsLocationsDataset.dropna(subset=['Latitude', 'Longitude']).drop_duplicates('AirQualityStationEoICode')\n",
    "#filter out stations not having location information\n",
    "airQualityStationsMeanReadings = airQualityStationsMeanReadings.loc[airQualityStationsMeanReadings['AirQualityStationEoICode'].isin(stationsLocationsDataset['AirQualityStationEoICode'].unique())]\n",
    "#Merge dataset\n",
    "airQualityStationsMeanReadings = pd.merge(stationsLocationsDataset[['AirQualityStationEoICode', 'Latitude', 'Longitude']], airQualityStationsMeanReadings, on='AirQualityStationEoICode')\n",
    "#filter out reading more than 2017\n",
    "airQualityStationsMeanReadings = airQualityStationsMeanReadings[airQualityStationsMeanReadings['Year'] < 2018]"
    "airQualityStationsMeanReadings.to_csv('E:/Research/Processing Datasets/AirQualityAggregatedDataset.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
