{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Script to populate pollutant concentrations on polutant release dataset\n",
    "import pandas as pd\n",
    "import time\n",
    "start = time.process_time()\n",
    "facilities = pd.read_csv('E:\\Research\\Datasets\\Pollutant emissions data\\E-PRTR_database_v16_csv\\dbo.PUBLISH_FACILITYREPORT.csv', encoding = \"ISO-8859-1\")\n",
    "facilities_nearby_stations = facilities_nearby_stations.dropna()\n",
    "facilities_nearby_stations = facilities_nearby_stations[(facilities_nearby_stations[['Dist_CO', 'Dist_O3', 'Dist_PM10', 'Dist_PM2.5', 'Dist_SO2']] < 50).all(1)]\n",
    "pollutant_release = pollutant_release.loc[pollutant_release['FacilityID'].isin(facilities_nearby_stations['FacilityID'].unique())]\n",
    "#Add new columns for pollutants\n",
    "pollutant_release = pd.concat([pollutant_release, pd.DataFrame(columns=[\"SO2\", \"C6H6\", \"CO\", \"PM10\", \"PM2.5\", \"O3\"])], sort=False)\n",
    "\n",
    "#Function to populate air quality pollutant concentration\n",
    "def populateAveragePolutantReading(row):\n",
    "    facilityId = row['FacilityID']\n",
    "    polutants = [\"SO2\", \"C6H6\", \"CO\", \"PM10\", \"PM2.5\", \"O3\"]\n",
    "    #For each pollutant and year, populate concentration\n",
    "    for pol in polutants:\n",
    "        reportingYearStations = facilities_nearby_stations.loc[(facilities_nearby_stations['FacilityID'] == facilityId) & (facilities_nearby_stations['Year'] == row['ReportingYear'])][pol]\n",
    "        if reportingYearStations.shape[0] > 0:\n",
    "            airQualitypollutants = airQualityStationsMeanReadings.loc[(airQualityStationsMeanReadings['AirQualityStationEoICode'] == reportingYearStations.values[0]) & (airQualityStationsMeanReadings['Year'] == row['ReportingYear'])]\n",
    "            if airQualitypollutants.shape[0] > 0:\n",
    "                row[pol] = airQualitypollutants[pol].values[0]\n",
    "    return row\n",
    "\n",
    "#populate for each facility\n",
    "pollutant_release_merged = pollutant_release.apply(lambda x: populateAveragePolutantReading(x), axis = 1)\n",
    "\n",
    "#Convert to csv\n",
    "pollutant_release_merged.to_csv(\"E:/Research/Processing Datasets/Pollutant_Release_Impact.csv\", index = False)\n",
    "print(\"Completed second merge in \", time.process_time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scripts to convert each pollutant group to a column \n",
    "pollutant_release_merged = pd.read_csv('E:/Research/Processing Datasets/Pollutant_Release_Impact.csv')\n",
    "groupedPollutantRelease = pollutant_release_merged.groupby(['FacilityID', 'ReportingYear'], as_index=False).agg({'SO2': 'mean', 'CO': 'mean', 'PM10': 'mean', 'O3': 'mean',  'PM2.5': 'mean'})\n",
    "\n",
    "def populatePollutantRelease(row):\n",
    "    \n",
    "    subset = pollutant_release_merged[(pollutant_release_merged['FacilityID'] == row['FacilityID']) & (pollutant_release_merged['ReportingYear'] == row['ReportingYear'])]\n",
    "    for releasedpol in subset['PollutantGroupCode'].unique():\n",
    "        row[releasedpol] = sum(subset[subset['PollutantGroupCode'] == releasedpol]['TotalQuantity'].values)\n",
    "    return row\n",
    "groupedPollutantRelease = groupedPollutantRelease.apply(lambda x: populatePollutantRelease(x), axis = 1)\n",
    "groupedPollutantRelease = pd.merge(groupedPollutantRelease,facilities[['FacilityID', 'CountryCode', 'CountryName']].drop_duplicates(), on = 'FacilityID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge pollutant releases based on city\n",
    "cityPopulatedFacilitiesDataset = facis[(facis['FacilityID'].isin(t2mp['FacilityID'].unique())) & (facis['City'].notna())]\n",
    "cityPopulatedFacilitiesDataset = cityPopulatedFacilitiesDataset.drop_duplicates('FacilityID')\n",
    "newmerge = pd.merge(t2mp, cityPopulatedFacilitiesDataset[['FacilityID', 'City']].drop_duplicates('FacilityID'), on='FacilityID')\n",
    "cityWiseMerged = newmerge.groupby(['City', 'CountryCode', 'ReportingYear']).agg({'CHLORG':'sum', 'GRHGAS':'sum', 'HEVMET':'sum', 'INORG':'sum', 'OTHGAS':'sum', 'OTHORG':'sum', 'PEST':'sum', 'SO2': 'mean', 'CO': 'mean', 'PM10': 'mean', 'O3': 'mean', 'PM2.5': 'mean'}).reset_index()\n",
    "filteredNonZeroData = cityWiseMerged[(cityWiseMerged['GRHGAS']>0) & (cityWiseMerged['HEVMET']>0) & (cityWiseMerged['OTHGAS']>0)].dropna()\n",
    "filteredNonZeroData.to_csv('cityWiseMergedFiftyKms.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
